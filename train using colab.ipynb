{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhWhuTnsMnKc"
      },
      "source": [
        "## getting the data and all the necessary text files and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij6sTs9SMbye"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88gkAyRTI03C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "c04178b9-9128-41e4-a609-f8e5f0cda6e5"
      },
      "source": [
        "#create a folder to store data\n",
        "!mkdir sample_data/IAM\n",
        "# copy the zip files that has data to this folder now\n",
        "!cp -nav \"/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/words zips\" /content/sample_data/IAM/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/words zips' -> '/content/sample_data/IAM/words zips'\n",
            "'/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/words zips/l-m.zip' -> '/content/sample_data/IAM/words zips/l-m.zip'\n",
            "'/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/words zips/g-h.zip' -> '/content/sample_data/IAM/words zips/g-h.zip'\n",
            "'/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/words zips/a.zip' -> '/content/sample_data/IAM/words zips/a.zip'\n",
            "'/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/words zips/n-p-r.zip' -> '/content/sample_data/IAM/words zips/n-p-r.zip'\n",
            "'/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/words zips/e-f.zip' -> '/content/sample_data/IAM/words zips/e-f.zip'\n",
            "'/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/words zips/j-k.zip' -> '/content/sample_data/IAM/words zips/j-k.zip'\n",
            "'/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/words zips/c-d.zip' -> '/content/sample_data/IAM/words zips/c-d.zip'\n",
            "'/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/words zips/b.zip' -> '/content/sample_data/IAM/words zips/b.zip'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAg7ktB-J2sp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "284de7ad-0c8f-4403-956b-ccf6aa20ec8a"
      },
      "source": [
        "# now unzip all the zips\n",
        "!unzip -nq /content/sample_data/IAM/'words zips'/\\*.zip -d /content/sample_data/IAM/'words zips'/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "8 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYUWzvw9KUgs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f279c09-f135-47d1-f860-49c7c8f576ba"
      },
      "source": [
        "# copy the ground truth texts file\n",
        "!cp -navr \"/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/words.txt\" \"/content/sample_data/IAM/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/words.txt' -> '/content/sample_data/IAM/words.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA476SmCKFbG"
      },
      "source": [
        "data_location = '/content/sample_data/IAM/words zips'\n",
        "words_txt_location = '/content/sample_data/IAM/words.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgyMtOTbMH3i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "1e8a6aa4-0a20-4837-bae4-47755ea31adb"
      },
      "source": [
        "# copy the splits files\n",
        "!cp -navr \"/content/drive/My Drive/Colab Notebooks/OCR on IAM/train_files.txt\" \"/content/sample_data/IAM/\"\n",
        "!cp -navr \"/content/drive/My Drive/Colab Notebooks/OCR on IAM/valid_files.txt\" \"/content/sample_data/IAM/\"\n",
        "!cp -navr \"/content/drive/My Drive/Colab Notebooks/OCR on IAM/test_files.txt\" \"/content/sample_data/IAM/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/My Drive/Colab Notebooks/OCR on IAM/train_files.txt' -> '/content/sample_data/IAM/train_files.txt'\n",
            "'/content/drive/My Drive/Colab Notebooks/OCR on IAM/valid_files.txt' -> '/content/sample_data/IAM/valid_files.txt'\n",
            "'/content/drive/My Drive/Colab Notebooks/OCR on IAM/test_files.txt' -> '/content/sample_data/IAM/test_files.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVsd-V5Xhobn"
      },
      "source": [
        "def get_paths_and_gts(partition_split_file):\n",
        "    \"\"\"\n",
        "    read a string like(which can be found in each line of words.txt file):\n",
        "    'a01-000u-00-00 ok 154 408 768 27 51 AT A'\n",
        "    and extract 'a01-000u-00-00': location of the image with sub-folders, to read it from the directories\n",
        "                'ok'            : processing status. ok means good, presumably.\n",
        "                'A'             : ground truth text\n",
        "\n",
        "    Then, pre-process using function defined above\n",
        "    \"\"\"\n",
        "    # a list to store paths to images and ground truth texts\n",
        "    paths_and_gts = []\n",
        "    \n",
        "    # open the file\n",
        "    with open(partition_split_file) as f:\n",
        "        # go through each line\n",
        "        for line in f:\n",
        "            # if a line is empty or commented with #, ignore that line\n",
        "            if not line or line.startswith('#'):\n",
        "                continue\n",
        "            \n",
        "            # in the text file, each line is seperated with '\\n', so `strip` first\n",
        "            # then string like 'a01-000u-00-00 ok 154 408 768 27 51 AT A' has to split to a list by spaces\n",
        "            line_split = line.strip().split(' ')\n",
        "            \n",
        "            # the first item of the list contains path information, so split that by '-'\n",
        "            directory_split = line_split[0].split('-')\n",
        "            \n",
        "            # now use all the above and concatenate to a string to make a path to an image\n",
        "            image_location = f'{data_location}/{directory_split[0]}/{directory_split[0]}-{directory_split[1]}/{line_split[0]}.png'\n",
        "            \n",
        "            # in a string like 'a01-000u-00-00 ok 154 408 768 27 51 AT A', text from 9th split is the ground truth text.\n",
        "            gt_text = ' '.join(line_split[8:])\n",
        "            \n",
        "            # ignore a sample(image and ground truth text), if the ground truth has more than 16 letters\n",
        "            # if len(gt_text) > 16:\n",
        "                # continue\n",
        "            \n",
        "            # now, append the image location and ground truth text of that image as a list to \n",
        "            paths_and_gts.append([image_location, gt_text])\n",
        "    \n",
        "    return paths_and_gts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awFnLaKjgng1"
      },
      "source": [
        "## pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtXXXRTagFnM"
      },
      "source": [
        "def add_padding(img, old_w, old_h, new_w, new_h):\n",
        "    h1, h2 = int((new_h - old_h) / 2), int((new_h - old_h) / 2) + old_h\n",
        "    w1, w2 = int((new_w - old_w) / 2), int((new_w - old_w) / 2) + old_w\n",
        "    img_pad = np.ones([new_h, new_w, 3]) * 255\n",
        "    img_pad[h1:h2, w1:w2, :] = img\n",
        "    return img_pad\n",
        "\n",
        "\n",
        "def fix_size(img, target_w, target_h):\n",
        "    h, w = img.shape[:2]\n",
        "    if w < target_w and h < target_h:\n",
        "        img = add_padding(img, w, h, target_w, target_h)\n",
        "    elif w >= target_w and h < target_h:\n",
        "        new_w = target_w\n",
        "        new_h = int(h * new_w / w)\n",
        "        new_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "        img = add_padding(new_img, new_w, new_h, target_w, target_h)\n",
        "    elif w < target_w and h >= target_h:\n",
        "        new_h = target_h\n",
        "        new_w = int(w * new_h / h)\n",
        "        new_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "        img = add_padding(new_img, new_w, new_h, target_w, target_h)\n",
        "    else:\n",
        "        \"\"\"w>=target_w and h>=target_h \"\"\"\n",
        "        ratio = max(w / target_w, h / target_h)\n",
        "        new_w = max(min(target_w, int(w / ratio)), 1)\n",
        "        new_h = max(min(target_h, int(h / ratio)), 1)\n",
        "        new_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "        img = add_padding(new_img, new_w, new_h, target_w, target_h)\n",
        "    return img\n",
        "\n",
        "\n",
        "def preprocess(path, img_w, img_h):\n",
        "    \"\"\" Pre-processing image for predicting \"\"\"\n",
        "    img = cv2.imread(path)\n",
        "    img = fix_size(img, img_w, img_h)\n",
        "\n",
        "    img = np.clip(img, 0, 255)\n",
        "    img = np.uint8(img)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    img = img.astype(np.float32)\n",
        "    img /= 255\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhoPnveagHTY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95a6866a-b9be-4882-a854-323253fedcc6"
      },
      "source": [
        "letters = [' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "           '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?',\n",
        "           'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
        "           'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
        "           'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
        "           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "num_classes = len(letters) + 1\n",
        "print(num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQsqbn2FM3V2"
      },
      "source": [
        "## generating all the pre-processed vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNBgqbnrhvt-"
      },
      "source": [
        "def text_to_labels(text):\n",
        "    return list(map(lambda x: letters.index(x), text))\n",
        "\n",
        "def labels_to_text(labels):\n",
        "    return ''.join(list(map(lambda x: letters[int(x)], labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKFzcbVQh3hV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ccf86710-3553-4473-d718-688f22a3a447"
      },
      "source": [
        "train_files = get_paths_and_gts('/content/sample_data/IAM/train_files.txt')\n",
        "valid_files = get_paths_and_gts('/content/sample_data/IAM/valid_files.txt')\n",
        "test_files = get_paths_and_gts('/content/sample_data/IAM/test_files.txt')\n",
        "len(train_files), len(valid_files), len(test_files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(87292, 4316, 4316)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-x3BG4yp7V0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "664f288d-60ab-42f4-915f-341876d8764a"
      },
      "source": [
        "# since this image is not readable is not readable, it has to be discarded.\n",
        "# ../words/r06/r06-022/r06-022-03-05.png'\n",
        "\n",
        "for index, (img_loc, gt_text) in enumerate(train_files):\n",
        "    if 'r06-022-03-05' in img_loc:\n",
        "        print(index)\n",
        "    else:\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piAUpeaVoTn6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e269a09e-768e-4bd5-c6e2-e2fdd16d1810"
      },
      "source": [
        "train_files[4576]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/sample_data/IAM/words zips/r06/r06-022/r06-022-03-05.png', 'more']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4qp1y6gojTh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "feb41dad-28cb-44e5-8de2-75e1ea90079a"
      },
      "source": [
        "del train_files[4576]\n",
        "print(train_files[4576])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/sample_data/IAM/words zips/g06/g06-042e/g06-042e-05-03.png', 'notice']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6f66gZXby6A"
      },
      "source": [
        "this is how images used for training will look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDFwXJNVhtgm"
      },
      "source": [
        "i = 21\n",
        "print(train_files[i][1])\n",
        "an_img = preprocess(path=train_files[i][0], img_w=128, img_h=64)\n",
        "plt.imshow(an_img.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sVUCBXqceh2"
      },
      "source": [
        "class TextImageGenerator:\n",
        "    \n",
        "    def __init__(self, data,\n",
        "                 img_w,\n",
        "                 img_h, \n",
        "                 batch_size, \n",
        "                 i_len,\n",
        "                 max_text_len):\n",
        "        \n",
        "        self.img_h = img_h\n",
        "        self.img_w = img_w\n",
        "        self.batch_size = batch_size\n",
        "        self.max_text_len = max_text_len\n",
        "        self.samples = data\n",
        "        self.n = len(self.samples)\n",
        "        self.i_len = i_len\n",
        "        self.indexes = list(range(self.n))\n",
        "        self.cur_index = 0\n",
        "        \n",
        "    def build_data(self):\n",
        "        self.imgs = np.zeros((self.n, self.img_h, self.img_w))\n",
        "        self.texts = []\n",
        "        for i, (img_filepath, text) in enumerate(self.samples):\n",
        "            img = preprocess(img_filepath, self.img_w, self.img_h)\n",
        "            self.imgs[i, :, :] = img\n",
        "            self.texts.append(text)\n",
        "    \n",
        "    def next_sample(self):\n",
        "        self.cur_index += 1\n",
        "        if self.cur_index >= self.n:\n",
        "            self.cur_index = 0\n",
        "            random.shuffle(self.indexes)\n",
        "        return self.imgs[self.indexes[self.cur_index]], self.texts[self.indexes[self.cur_index]]\n",
        "    \n",
        "    def next_batch(self):\n",
        "        while True:\n",
        "            # width and height are backwards from typical Keras convention\n",
        "            # because width is the time dimension when it gets fed into the RNN\n",
        "            X_data = np.ones([self.batch_size, self.img_w, self.img_h, 1])\n",
        "            Y_data = np.zeros([self.batch_size, self.max_text_len])\n",
        "            input_length = np.ones((self.batch_size, 1)) * self.i_len\n",
        "            label_length = np.zeros((self.batch_size, 1))\n",
        "                                   \n",
        "            for i in range(self.batch_size):\n",
        "                img, text = self.next_sample()\n",
        "                img = img.T\n",
        "                img = np.expand_dims(img, -1)\n",
        "                X_data[i] = img\n",
        "                Y_data[i, :len(text)] = text_to_labels(text)\n",
        "                label_length[i] = len(text)\n",
        "                \n",
        "            inputs = [X_data, Y_data, input_length, label_length]\n",
        "            outputs = np.zeros([self.batch_size])\n",
        "            yield (inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiY8tC1gcqU9"
      },
      "source": [
        "batch_size = 64\n",
        "input_length = 30\n",
        "max_text_len = 16\n",
        "img_w = 128\n",
        "img_h = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "191h4j01ciIM"
      },
      "source": [
        "train_data = TextImageGenerator(train_files, img_w, img_h, batch_size, input_length, max_text_len)\n",
        "train_data.build_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48MDmv6VfEmJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "517000cb-e65d-4ebd-a5a4-0d5be4ad858e"
      },
      "source": [
        "train_data.imgs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(87291, 64, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9IuosR1IT8B"
      },
      "source": [
        "validation_data = TextImageGenerator(valid_files, img_w, img_h, batch_size, input_length, max_text_len)\n",
        "validation_data.build_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sRlzy4COkvW"
      },
      "source": [
        "# making model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74GR3lUAS2zf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c5c083f-0743-43f3-f5fe-67cf21148a66"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import backend as tf_keras_backend\n",
        "\n",
        "tf_keras_backend.set_image_data_format('channels_last')\n",
        "tf_keras_backend.image_data_format()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'channels_last'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtu1k2kcyuog",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a4ead206-b2e1-4def-e17a-98b61d2d72c7"
      },
      "source": [
        "input_data = layers.Input(name='the_input', shape=(128,64,1), dtype='float32')  # (None, 128, 64, 1)\n",
        "\n",
        "# Convolution layer (VGG)\n",
        "iam_layers = layers.Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(input_data)\n",
        "iam_layers = layers.BatchNormalization()(iam_layers)\n",
        "iam_layers = layers.Activation('relu')(iam_layers)\n",
        "iam_layers = layers.MaxPooling2D(pool_size=(2, 2), name='max1')(iam_layers)  # (None,64, 32, 64)\n",
        "\n",
        "iam_layers = layers.Conv2D(128, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(iam_layers)\n",
        "iam_layers = layers.BatchNormalization()(iam_layers)\n",
        "iam_layers = layers.Activation('relu')(iam_layers)\n",
        "iam_layers = layers.MaxPooling2D(pool_size=(2, 2), name='max2')(iam_layers)\n",
        "\n",
        "iam_layers = layers.Conv2D(256, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(iam_layers)\n",
        "iam_layers = layers.BatchNormalization()(iam_layers)\n",
        "iam_layers = layers.Activation('relu')(iam_layers)\n",
        "iam_layers = layers.Conv2D(256, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(iam_layers)\n",
        "iam_layers = layers.BatchNormalization()(iam_layers)\n",
        "iam_layers = layers.Activation('relu')(iam_layers)\n",
        "iam_layers = layers.MaxPooling2D(pool_size=(1, 2), name='max3')(iam_layers)  # (None, 32, 8, 256)\n",
        "\n",
        "iam_layers = layers.Conv2D(512, (3, 3), padding='same', name='conv5', kernel_initializer='he_normal')(iam_layers)\n",
        "iam_layers = layers.BatchNormalization()(iam_layers)\n",
        "iam_layers = layers.Activation('relu')(iam_layers)\n",
        "iam_layers = layers.Conv2D(512, (3, 3), padding='same', name='conv6')(iam_layers)\n",
        "iam_layers = layers.BatchNormalization()(iam_layers)\n",
        "iam_layers = layers.Activation('relu')(iam_layers)\n",
        "iam_layers = layers.MaxPooling2D(pool_size=(1, 2), name='max4')(iam_layers)\n",
        "\n",
        "iam_layers = layers.Conv2D(512, (2, 2), padding='same', kernel_initializer='he_normal', name='con7')(iam_layers)\n",
        "iam_layers = layers.BatchNormalization()(iam_layers)\n",
        "iam_layers = layers.Activation('relu')(iam_layers)\n",
        "\n",
        "# CNN to RNN\n",
        "iam_layers = layers.Reshape(target_shape=((32, 2048)), name='reshape')(iam_layers)\n",
        "iam_layers = layers.Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(iam_layers)\n",
        "\n",
        "# RNN layer\n",
        "# layer ten\n",
        "iam_layers = layers.Bidirectional(layers.LSTM(units=256, return_sequences=True))(iam_layers)\n",
        "# layer nine\n",
        "iam_layers = layers.Bidirectional(layers.LSTM(units=256, return_sequences=True))(iam_layers)\n",
        "iam_layers = layers.BatchNormalization()(iam_layers)\n",
        "\n",
        "# transforms RNN output to character activations:\n",
        "iam_layers = layers.Dense(80, kernel_initializer='he_normal', name='dense2')(iam_layers)\n",
        "iam_outputs = layers.Activation('softmax', name='softmax')(iam_layers)\n",
        "\n",
        "labels = layers.Input(name='the_labels', shape=[16], dtype='float32')\n",
        "input_length = layers.Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = layers.Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "\n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    # the 2 is critical here since the first couple outputs of the RNN\n",
        "    # tend to be garbage:\n",
        "    y_pred = y_pred[:, 2:, :]\n",
        "    return tf_keras_backend.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        "\n",
        "\n",
        "# loss function\n",
        "loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([iam_outputs, labels, input_length, label_length])\n",
        "\n",
        "model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "the_input (InputLayer)          [(None, 128, 64, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 128, 64, 64)  640         the_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 128, 64, 64)  256         conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 128, 64, 64)  0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max1 (MaxPooling2D)             (None, 64, 32, 64)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2 (Conv2D)                  (None, 64, 32, 128)  73856       max1[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 64, 32, 128)  512         conv2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 64, 32, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max2 (MaxPooling2D)             (None, 32, 16, 128)  0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv3 (Conv2D)                  (None, 32, 16, 256)  295168      max2[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 16, 256)  1024        conv3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 16, 256)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4 (Conv2D)                  (None, 32, 16, 256)  590080      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 16, 256)  1024        conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 16, 256)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max3 (MaxPooling2D)             (None, 32, 8, 256)   0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv5 (Conv2D)                  (None, 32, 8, 512)   1180160     max3[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 8, 512)   2048        conv5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 8, 512)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv6 (Conv2D)                  (None, 32, 8, 512)   2359808     activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 8, 512)   2048        conv6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 8, 512)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max4 (MaxPooling2D)             (None, 32, 4, 512)   0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "con7 (Conv2D)                   (None, 32, 4, 512)   1049088     max4[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 4, 512)   2048        con7[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 4, 512)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 32, 2048)     0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense1 (Dense)                  (None, 32, 64)       131136      reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 32, 512)      657408      dense1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 32, 512)      1574912     bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 512)      2048        bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense2 (Dense)                  (None, 32, 80)       41040       batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Activation)            (None, 32, 80)       0           dense2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "the_labels (InputLayer)         [(None, 16)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_length (InputLayer)       [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "label_length (InputLayer)       [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
            "                                                                 the_labels[0][0]                 \n",
            "                                                                 input_length[0][0]               \n",
            "                                                                 label_length[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 7,964,304\n",
            "Trainable params: 7,958,800\n",
            "Non-trainable params: 5,504\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVhJC5rqTgGg"
      },
      "source": [
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
        "# you will know, why there aren't any metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P5u37CzUfF7"
      },
      "source": [
        "import time\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class EpochTimeHistory(Callback):\n",
        "    \"\"\"\n",
        "    a custom callback to print the time(in minutes, to console) each epoch took during.\n",
        "    \"\"\"\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.train_epoch_times = []\n",
        "        self.valid_epoch_times = []\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        cur_epoch_time = round((time.time() - self.epoch_time_start)/60, 4)\n",
        "        self.train_epoch_times.append(cur_epoch_time)\n",
        "        print(\" ;epoch {0} took {1} minutes.\".format(epoch+1, cur_epoch_time))\n",
        "\n",
        "\n",
        "    def on_test_begin(self, logs={}):\n",
        "        self.test_time_start = time.time()\n",
        "\n",
        "    def on_test_end(self, logs={}):\n",
        "        cur_test_time = round((time.time() - self.test_time_start)/60, 4)\n",
        "        self.valid_epoch_times.append(cur_test_time)\n",
        "        print(\" ;validation took {} minutes.\".format(cur_test_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT6aB8EnUffU"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "model_save_cb = ModelCheckpoint(filepath='/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch{epoch:02d}-val_loss{val_loss:.3f}.h5',\n",
        "                                verbose=1, save_best_only=False, monitor='val_loss', save_weights_only=False)\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=0, mode='min')\n",
        "# reduce_learning_rate_cb = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, cooldown=2, min_lr=0.00001, verbose=1)\n",
        "epoch_times = EpochTimeHistory()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gZCj-MrehK1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20b83af5-faf7-4c86-d79c-f337fef0985c"
      },
      "source": [
        "batch_size, train_data.n, validation_data.n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 87291, 4316)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4b4BM1_T7-O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d69b5e6a-1932-49e7-d12f-eec999aeeb81"
      },
      "source": [
        "history_model_3 = model.fit(train_data.next_batch(),\n",
        "                            validation_data=validation_data.next_batch(),\n",
        "                            steps_per_epoch=train_data.n/batch_size,\n",
        "                            validation_steps=validation_data.n // batch_size,\n",
        "                            epochs=30,\n",
        "                            callbacks=[earlystop, model_save_cb, epoch_times])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 7.3719 ;validation took 0.1656 minutes.\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch01-val_loss3.449.h5\n",
            " ;epoch 1 took 7.2078 minutes.\n",
            "1364/1363 [==============================] - 415s 304ms/step - loss: 7.3719 - val_loss: 3.4487\n",
            "Epoch 2/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 2.2471 ;validation took 0.1339 minutes.\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch02-val_loss2.158.h5\n",
            " ;epoch 2 took 6.8968 minutes.\n",
            "1364/1363 [==============================] - 414s 303ms/step - loss: 2.2471 - val_loss: 2.1582\n",
            "Epoch 3/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 1.6207 ;validation took 0.1337 minutes.\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch03-val_loss1.643.h5\n",
            " ;epoch 3 took 6.8964 minutes.\n",
            "1364/1363 [==============================] - 413s 303ms/step - loss: 1.6207 - val_loss: 1.6430\n",
            "Epoch 4/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 1.3192 ;validation took 0.1323 minutes.\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch04-val_loss1.405.h5\n",
            " ;epoch 4 took 6.8897 minutes.\n",
            "1364/1363 [==============================] - 413s 303ms/step - loss: 1.3192 - val_loss: 1.4053\n",
            "Epoch 5/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 1.0948 ;validation took 0.1353 minutes.\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch05-val_loss1.070.h5\n",
            " ;epoch 5 took 6.9132 minutes.\n",
            "1364/1363 [==============================] - 415s 304ms/step - loss: 1.0948 - val_loss: 1.0700\n",
            "Epoch 6/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.9156 ;validation took 0.1322 minutes.\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch06-val_loss1.222.h5\n",
            " ;epoch 6 took 6.8933 minutes.\n",
            "1364/1363 [==============================] - 413s 303ms/step - loss: 0.9156 - val_loss: 1.2221\n",
            "Epoch 7/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.7822 ;validation took 0.1332 minutes.\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch07-val_loss0.692.h5\n",
            " ;epoch 7 took 6.891 minutes.\n",
            "1364/1363 [==============================] - 413s 303ms/step - loss: 0.7822 - val_loss: 0.6921\n",
            "Epoch 8/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.6488 ;validation took 0.1325 minutes.\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch08-val_loss0.832.h5\n",
            " ;epoch 8 took 6.8755 minutes.\n",
            "1364/1363 [==============================] - 412s 302ms/step - loss: 0.6488 - val_loss: 0.8324\n",
            "Epoch 9/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.5596 ;validation took 0.1338 minutes.\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch09-val_loss1.825.h5\n",
            " ;epoch 9 took 6.8894 minutes.\n",
            "1364/1363 [==============================] - 413s 303ms/step - loss: 0.5596 - val_loss: 1.8247\n",
            "Epoch 10/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.4838 ;validation took 0.1339 minutes.\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch10-val_loss0.509.h5\n",
            " ;epoch 10 took 6.8808 minutes.\n",
            "1364/1363 [==============================] - 413s 302ms/step - loss: 0.4838 - val_loss: 0.5093\n",
            "Epoch 11/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.4115 ;validation took 0.133 minutes.\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch11-val_loss0.699.h5\n",
            " ;epoch 11 took 6.892 minutes.\n",
            "1364/1363 [==============================] - 413s 303ms/step - loss: 0.4115 - val_loss: 0.6988\n",
            "Epoch 12/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.3598 ;validation took 0.1344 minutes.\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch12-val_loss3.366.h5\n",
            " ;epoch 12 took 6.9025 minutes.\n",
            "1364/1363 [==============================] - 414s 303ms/step - loss: 0.3598 - val_loss: 3.3660\n",
            "Epoch 13/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.3328 ;validation took 0.1343 minutes.\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch13-val_loss0.430.h5\n",
            " ;epoch 13 took 6.9143 minutes.\n",
            "1364/1363 [==============================] - 415s 304ms/step - loss: 0.3328 - val_loss: 0.4303\n",
            "Epoch 14/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.2973 ;validation took 0.133 minutes.\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch14-val_loss0.562.h5\n",
            " ;epoch 14 took 6.9122 minutes.\n",
            "1364/1363 [==============================] - 414s 304ms/step - loss: 0.2973 - val_loss: 0.5624\n",
            "Epoch 15/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.2623 ;validation took 0.1339 minutes.\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch15-val_loss0.573.h5\n",
            " ;epoch 15 took 6.886 minutes.\n",
            "1364/1363 [==============================] - 413s 303ms/step - loss: 0.2623 - val_loss: 0.5728\n",
            "Epoch 16/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.2458 ;validation took 0.1348 minutes.\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch16-val_loss0.506.h5\n",
            " ;epoch 16 took 6.8921 minutes.\n",
            "1364/1363 [==============================] - 413s 303ms/step - loss: 0.2458 - val_loss: 0.5058\n",
            "Epoch 17/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.2176 ;validation took 0.1336 minutes.\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch17-val_loss0.418.h5\n",
            " ;epoch 17 took 6.8881 minutes.\n",
            "1364/1363 [==============================] - 413s 303ms/step - loss: 0.2176 - val_loss: 0.4177\n",
            "Epoch 18/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.2186 ;validation took 0.1346 minutes.\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch18-val_loss0.407.h5\n",
            " ;epoch 18 took 6.8995 minutes.\n",
            "1364/1363 [==============================] - 414s 303ms/step - loss: 0.2186 - val_loss: 0.4075\n",
            "Epoch 19/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.1989 ;validation took 0.1345 minutes.\n",
            "\n",
            "Epoch 00019: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch19-val_loss0.380.h5\n",
            " ;epoch 19 took 6.909 minutes.\n",
            "1364/1363 [==============================] - 414s 304ms/step - loss: 0.1989 - val_loss: 0.3801\n",
            "Epoch 20/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.1806 ;validation took 0.1332 minutes.\n",
            "\n",
            "Epoch 00020: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch20-val_loss0.491.h5\n",
            " ;epoch 20 took 6.8969 minutes.\n",
            "1364/1363 [==============================] - 414s 303ms/step - loss: 0.1806 - val_loss: 0.4906\n",
            "Epoch 21/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.1880 ;validation took 0.1327 minutes.\n",
            "\n",
            "Epoch 00021: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch21-val_loss0.419.h5\n",
            " ;epoch 21 took 6.8972 minutes.\n",
            "1364/1363 [==============================] - 414s 303ms/step - loss: 0.1880 - val_loss: 0.4189\n",
            "Epoch 22/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.1742 ;validation took 0.1335 minutes.\n",
            "\n",
            "Epoch 00022: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch22-val_loss0.309.h5\n",
            " ;epoch 22 took 6.8964 minutes.\n",
            "1364/1363 [==============================] - 413s 303ms/step - loss: 0.1742 - val_loss: 0.3087\n",
            "Epoch 23/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.1587 ;validation took 0.1345 minutes.\n",
            "\n",
            "Epoch 00023: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch23-val_loss26.979.h5\n",
            " ;epoch 23 took 6.9151 minutes.\n",
            "1364/1363 [==============================] - 415s 304ms/step - loss: 0.1587 - val_loss: 26.9786\n",
            "Epoch 24/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.1627 ;validation took 0.1337 minutes.\n",
            "\n",
            "Epoch 00024: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch24-val_loss0.242.h5\n",
            " ;epoch 24 took 6.9653 minutes.\n",
            "1364/1363 [==============================] - 418s 306ms/step - loss: 0.1627 - val_loss: 0.2417\n",
            "Epoch 25/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.1415 ;validation took 0.1348 minutes.\n",
            "\n",
            "Epoch 00025: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch25-val_loss0.349.h5\n",
            " ;epoch 25 took 6.9589 minutes.\n",
            "1364/1363 [==============================] - 417s 306ms/step - loss: 0.1415 - val_loss: 0.3488\n",
            "Epoch 26/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.1511 ;validation took 0.135 minutes.\n",
            "\n",
            "Epoch 00026: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch26-val_loss4.603.h5\n",
            " ;epoch 26 took 6.9743 minutes.\n",
            "1364/1363 [==============================] - 418s 307ms/step - loss: 0.1511 - val_loss: 4.6032\n",
            "Epoch 27/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.1434 ;validation took 0.134 minutes.\n",
            "\n",
            "Epoch 00027: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch27-val_loss0.421.h5\n",
            " ;epoch 27 took 6.9606 minutes.\n",
            "1364/1363 [==============================] - 417s 306ms/step - loss: 0.1434 - val_loss: 0.4214\n",
            "Epoch 28/30\n",
            "1364/1363 [==============================] - ETA: 0s - loss: 0.1281 ;validation took 0.1358 minutes.\n",
            "\n",
            "Epoch 00028: saving model to /content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch28-val_loss0.300.h5\n",
            " ;epoch 28 took 6.9651 minutes.\n",
            "1364/1363 [==============================] - 418s 306ms/step - loss: 0.1281 - val_loss: 0.2998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1lgH_FKVEsv"
      },
      "source": [
        "model.save(filepath='/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-model-after-3rd-session.h5', overwrite=False, include_optimizer=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EycN6u09NzVx"
      },
      "source": [
        "## plotting model statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSuyheptN1nS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4bd46308-42f9-4537-f19f-41db94325f47"
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJOuTJN_W0br",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "184f0917-99f6-47ad-f697-99ae1cf8f429"
      },
      "source": [
        "sns.set(rc={'figure.figsize':(14,7)})\n",
        "plt.plot(history_model_3.history['loss'])\n",
        "plt.plot(history_model_3.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_oypp877bqx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f4eec7d-5de0-4d41-9e91-eaf86de72926"
      },
      "source": [
        "history_model_3.history.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'val_loss'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-ll8y2i7Dux"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt8HYO4N78U8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "205d066f-c719-4996-fd07-9b1abef00b84"
      },
      "source": [
        "loss_df = pd.DataFrame(data=history_model_3.history)\n",
        "loss_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.371885</td>\n",
              "      <td>3.448700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.247088</td>\n",
              "      <td>2.158155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.620672</td>\n",
              "      <td>1.642994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.319233</td>\n",
              "      <td>1.405329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.094819</td>\n",
              "      <td>1.069993</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       loss  val_loss\n",
              "0  7.371885  3.448700\n",
              "1  2.247088  2.158155\n",
              "2  1.620672  1.642994\n",
              "3  1.319233  1.405329\n",
              "4  1.094819  1.069993"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdDp-8jG8Gd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "8b73c00c-6b91-4ad8-828d-32fcafab7f96"
      },
      "source": [
        "loss_df.sort_values(by=['loss', 'val_loss']).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.128097</td>\n",
              "      <td>0.299811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.141549</td>\n",
              "      <td>0.348824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.143362</td>\n",
              "      <td>0.421445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.151138</td>\n",
              "      <td>4.603170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.158717</td>\n",
              "      <td>26.978632</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss   val_loss\n",
              "27  0.128097   0.299811\n",
              "24  0.141549   0.348824\n",
              "26  0.143362   0.421445\n",
              "25  0.151138   4.603170\n",
              "22  0.158717  26.978632"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uObAhBrd8hwI"
      },
      "source": [
        "so, let's use the weights of 27th epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey46Cdq_SpJh"
      },
      "source": [
        "## predictions on test set and calculating WER & CER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rebDDZj7YeXy"
      },
      "source": [
        "make predictions using weights from various checkpoints, since that's the epoch with lowest train and validation error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajetvs96cSEk"
      },
      "source": [
        "sns.reset_orig()\n",
        "plt.figure(figsize=(3, 6))\n",
        "i = 1\n",
        "print(test_files[i][1])\n",
        "temp_processed_image = preprocess(path=test_files[i][0], img_w=128, img_h=64)\n",
        "plt.imshow(temp_processed_image.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmlTvqlec6lg"
      },
      "source": [
        "test_images_processed = []\n",
        "original_test_texts = []\n",
        "for _, (test_image_path, original_test_text) in enumerate(test_files):\n",
        "     temp_processed_image = preprocess(path=test_image_path, img_w=128, img_h=64)\n",
        "     test_images_processed.append(temp_processed_image.T)\n",
        "     original_test_texts.append(original_test_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myzKjOfxfWT9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "3d92f50b-049b-43a1-e436-9427b01bb905"
      },
      "source": [
        "print(len(test_files))\n",
        "print(len(test_images_processed))\n",
        "print(len(original_test_texts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4316\n",
            "4316\n",
            "4316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBy4cTXoei-z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c99eb4b2-ebfb-43f9-98ad-8cf6b33ce11d"
      },
      "source": [
        "test_images_processed = np.array(test_images_processed)\n",
        "test_images_processed.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4316, 128, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5n3s_z-f00S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1cda6cf2-d275-425b-f5ac-eb0323397e3c"
      },
      "source": [
        "test_images_processed = test_images_processed.reshape(4316, 128, 64, 1)\n",
        "test_images_processed.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4316, 128, 64, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReGR1lE3kvp0"
      },
      "source": [
        "## with weights of final epoch (the one with the least loss on train data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGNSsMJ3YdiK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ded6e7fb-4ed7-46e3-c88c-9b21ad6659c8"
      },
      "source": [
        "iam_model_pred = None\n",
        "iam_model_pred = Model(inputs=input_data, outputs=iam_outputs)\n",
        "iam_model_pred.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "the_input (InputLayer)       [(None, 128, 64, 1)]      0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 128, 64, 64)       640       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128, 64, 64)       256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128, 64, 64)       0         \n",
            "_________________________________________________________________\n",
            "max1 (MaxPooling2D)          (None, 64, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 64, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 64, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "max2 (MaxPooling2D)          (None, 32, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 32, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, 32, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 32, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "max3 (MaxPooling2D)          (None, 32, 8, 256)        0         \n",
            "_________________________________________________________________\n",
            "conv5 (Conv2D)               (None, 32, 8, 512)        1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 32, 8, 512)        2048      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 32, 8, 512)        0         \n",
            "_________________________________________________________________\n",
            "conv6 (Conv2D)               (None, 32, 8, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 32, 8, 512)        2048      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 32, 8, 512)        0         \n",
            "_________________________________________________________________\n",
            "max4 (MaxPooling2D)          (None, 32, 4, 512)        0         \n",
            "_________________________________________________________________\n",
            "con7 (Conv2D)                (None, 32, 4, 512)        1049088   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 32, 4, 512)        2048      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 32, 4, 512)        0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 32, 2048)          0         \n",
            "_________________________________________________________________\n",
            "dense1 (Dense)               (None, 32, 64)            131136    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 32, 512)           657408    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 32, 512)           1574912   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32, 512)           2048      \n",
            "_________________________________________________________________\n",
            "dense2 (Dense)               (None, 32, 80)            41040     \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 32, 80)            0         \n",
            "=================================================================\n",
            "Total params: 7,964,304\n",
            "Trainable params: 7,958,800\n",
            "Non-trainable params: 5,504\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm3TPzRXasYp"
      },
      "source": [
        "iam_model_pred.load_weights(filepath='/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-model-after-3rd-session.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBJ9W1ysa5PE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6169a9e5-ffca-4e58-f374-b68ddabafa6d"
      },
      "source": [
        "test_predictions_encoded = iam_model_pred.predict(x=test_images_processed)\n",
        "test_predictions_encoded.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4316, 32, 80)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojaGQiMYgb1M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "069b93d4-f0be-4fed-d420-259977ab00f9"
      },
      "source": [
        "# use CTC decoder to decode to text\n",
        "test_predictions_decoded = tf_keras_backend.get_value(tf_keras_backend.ctc_decode(test_predictions_encoded,\n",
        "                                                                                  input_length = np.ones(test_predictions_encoded.shape[0])*test_predictions_encoded.shape[1],\n",
        "                                                                                  greedy=True)[0][0])\n",
        "test_predictions_decoded.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4316, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u0TO-Y3gvtA"
      },
      "source": [
        "def numbered_array_to_text(numbered_array):\n",
        "    numbered_array = numbered_array[numbered_array != -1]\n",
        "    return \"\".join(letters[i] for i in numbered_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttNBrifAiL-k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "f96ebf30-08aa-470a-b599-8b6e7ac8dba2"
      },
      "source": [
        "for i in range(10):\n",
        "    print(\"original_text = \", original_test_texts[i])\n",
        "    print(\"predicted text = \", numbered_array_to_text(test_predictions_decoded[i]))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original_text =  certain\n",
            "predicted text =  ;certain\n",
            "\n",
            "original_text =  biographies\n",
            "predicted text =  ?biographies\n",
            "\n",
            "original_text =  all\n",
            "predicted text =  !&all\n",
            "\n",
            "original_text =  It\n",
            "predicted text =  It\n",
            "\n",
            "original_text =  I\n",
            "predicted text =  ?I\n",
            "\n",
            "original_text =  show\n",
            "predicted text =  #show\n",
            "\n",
            "original_text =  Kings\n",
            "predicted text =  XKings\n",
            "\n",
            "original_text =  A\n",
            "predicted text =  A\n",
            "\n",
            "original_text =  the\n",
            "predicted text =  7the\n",
            "\n",
            "original_text =  and\n",
            "predicted text =  !3and\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QYbs62Uk2sP"
      },
      "source": [
        "### with the weight of epoch 24 (the one with the least loss on validation data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEh3Z1zKlCoe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48676325-b99a-49aa-c250-5efec63af1bc"
      },
      "source": [
        "iam_model_pred = None\n",
        "iam_model_pred = Model(inputs=input_data, outputs=iam_outputs)\n",
        "iam_model_pred.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "the_input (InputLayer)       [(None, 128, 64, 1)]      0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 128, 64, 64)       640       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128, 64, 64)       256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128, 64, 64)       0         \n",
            "_________________________________________________________________\n",
            "max1 (MaxPooling2D)          (None, 64, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 64, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 64, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "max2 (MaxPooling2D)          (None, 32, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 32, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, 32, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 32, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "max3 (MaxPooling2D)          (None, 32, 8, 256)        0         \n",
            "_________________________________________________________________\n",
            "conv5 (Conv2D)               (None, 32, 8, 512)        1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 32, 8, 512)        2048      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 32, 8, 512)        0         \n",
            "_________________________________________________________________\n",
            "conv6 (Conv2D)               (None, 32, 8, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 32, 8, 512)        2048      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 32, 8, 512)        0         \n",
            "_________________________________________________________________\n",
            "max4 (MaxPooling2D)          (None, 32, 4, 512)        0         \n",
            "_________________________________________________________________\n",
            "con7 (Conv2D)                (None, 32, 4, 512)        1049088   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 32, 4, 512)        2048      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 32, 4, 512)        0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 32, 2048)          0         \n",
            "_________________________________________________________________\n",
            "dense1 (Dense)               (None, 32, 64)            131136    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 32, 512)           657408    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 32, 512)           1574912   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32, 512)           2048      \n",
            "_________________________________________________________________\n",
            "dense2 (Dense)               (None, 32, 80)            41040     \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 32, 80)            0         \n",
            "=================================================================\n",
            "Total params: 7,964,304\n",
            "Trainable params: 7,958,800\n",
            "Non-trainable params: 5,504\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pOtYeElk8kz"
      },
      "source": [
        "iam_model_pred.load_weights(filepath='/content/drive/My Drive/Colab Notebooks/OCR on IAM/data/two-weights-epoch24-val_loss0.242.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lCXMAgElHtQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "7fef70da-ff74-4a3f-9b91-b95239e93b82"
      },
      "source": [
        "test_predictions_encoded = iam_model_pred.predict(x=test_images_processed)\n",
        "# use CTC decoder to decode to text\n",
        "test_predictions_decoded = tf_keras_backend.get_value(tf_keras_backend.ctc_decode(test_predictions_encoded,\n",
        "                                                                                  input_length = np.ones(test_predictions_encoded.shape[0])*test_predictions_encoded.shape[1],\n",
        "                                                                                  greedy=True)[0][0])\n",
        "test_predictions_decoded.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:5871: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4316, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}